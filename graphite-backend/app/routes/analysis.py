"""
æ•°æ®åˆ†æAPIè·¯ç”±
æä¾›æ•°æ®æŸ¥è¯¢ã€æ¸…æ´—å’Œå›å½’åˆ†æåŠŸèƒ½
"""

from flask import Blueprint, request, jsonify
from flask_jwt_extended import jwt_required, get_jwt_identity
from sqlalchemy import text
from app import db
from app.utils.decorators import role_required
from app.utils.data_cleaning import clean_analysis_data, generate_cleaning_report
import numpy as np
from scipy import stats

# åˆ›å»ºè“å›¾ï¼ˆä¸è®¾ç½®url_prefixï¼Œåœ¨__init__.pyä¸­ç»Ÿä¸€è®¾ç½®ï¼‰
analysis_bp = Blueprint('analysis', __name__)

# å­—æ®µå…ƒæ•°æ®ï¼ˆä¸­æ–‡åç§°å’Œå•ä½ï¼‰
FIELD_METADATA = {
    # ç¢³åŒ–å‚æ•° - âœ… ä¿®æ­£ä¸ºæ•°æ®åº“å®é™…å­—æ®µå
    'carbon_max_temp': {'label': 'ç¢³åŒ–æœ€é«˜æ¸©åº¦', 'unit': 'â„ƒ'},
    'carbon_total_time': {'label': 'ç¢³åŒ–æ€»æ—¶é•¿', 'unit': 'min'},
    'carbon_yield_rate': {'label': 'ç¢³åŒ–æ”¶ç‡', 'unit': '%'},
    
    # çŸ³å¢¨åŒ–å‚æ•° - âœ… ä¿®æ­£ä¸ºæ•°æ®åº“å®é™…å­—æ®µå
    'graphite_max_temp': {'label': 'çŸ³å¢¨åŒ–æœ€é«˜æ¸©åº¦', 'unit': 'â„ƒ'},
    'graphite_total_time': {'label': 'çŸ³å¢¨åŒ–æ€»æ—¶é•¿', 'unit': 'min'},
    'graphite_yield_rate': {'label': 'çŸ³å¢¨åŒ–æ”¶ç‡', 'unit': '%'},
    
    # æˆå“å‚æ•° - âœ… åªä¿ç•™æ•°æ®åº“å­˜åœ¨çš„å­—æ®µ
    'thermal_conductivity': {'label': 'å¯¼çƒ­ç³»æ•°', 'unit': 'W/mÂ·K'},
    'avg_density': {'label': 'å¹³å‡å¯†åº¦', 'unit': 'g/cmÂ³'},
    'avg_thickness': {'label': 'å¹³å‡åšåº¦', 'unit': 'Î¼m'},
    'shrinkage_ratio': {'label': 'æ”¶ç¼©æ¯”', 'unit': '%'},
    'cohesion': {'label': 'å†…èšåŠ›', 'unit': 'MPa'},
    'peel_strength': {'label': 'å‰¥ç¦»å¼ºåº¦', 'unit': 'N/cm'},
    
    # PIè†œå‚æ•°
    'pi_film_thickness': {'label': 'PIè†œåšåº¦', 'unit': 'Î¼m'}
}


@analysis_bp.route('/data', methods=['GET'])
@jwt_required()
@role_required(['admin', 'engineer'])
def get_analysis_data():
    """
    è·å–åˆ†ææ•°æ®ï¼ˆä½¿ç”¨ v_experiment_full è§†å›¾ï¼‰
    """
    # ===== è°ƒè¯•æ‰“å°å¼€å§‹ =====
    print("=" * 60)
    print("ğŸ“Š [DEBUG] get_analysis_data å‡½æ•°è¢«è°ƒç”¨")
    print(f"ğŸ“Š [DEBUG] è¯·æ±‚å‚æ•°: {dict(request.args)}")
    print("=" * 60)
    
    try:
        # 1. è·å–å¿…å¡«å‚æ•°
        x_field = request.args.get('x_field')
        y_field = request.args.get('y_field')
        
        print(f"ğŸ“Š [DEBUG] x_field={x_field}, y_field={y_field}")
        
        if not x_field or not y_field:
            print("âŒ [DEBUG] ç¼ºå°‘å¿…å¡«å­—æ®µ")
            return jsonify({
                'error': 'Missing required fields',
                'message': 'è¯·é€‰æ‹©Xè½´å’ŒYè½´å­—æ®µ'
            }), 400
        
        # 2. éªŒè¯å­—æ®µæ˜¯å¦å­˜åœ¨ï¼ˆç™½åå•æ£€æŸ¥ï¼Œé˜²æ­¢SQLæ³¨å…¥ï¼‰
        if x_field not in FIELD_METADATA or y_field not in FIELD_METADATA:
            print(f"âŒ [DEBUG] å­—æ®µä¸å­˜åœ¨: x_field={x_field}, y_field={y_field}")
            return jsonify({
                'error': 'Invalid field',
                'message': 'é€‰æ‹©çš„å­—æ®µä¸å­˜åœ¨'
            }), 400
        
        print("âœ… [DEBUG] å­—æ®µéªŒè¯é€šè¿‡")
        
        # 3. æ„å»ºåŸºç¡€ SQLï¼ˆâœ… ä¿®å¤ï¼šæ”¹ä¸º submitted, completedï¼‰
        query = f"""
            SELECT 
                experiment_code,
                {x_field} as x_value,
                {y_field} as y_value
            FROM v_experiment_full
            WHERE {x_field} IS NOT NULL 
              AND {y_field} IS NOT NULL
              AND status IN ('submitted', 'completed')
        """
        
        print(f"ğŸ“Š [DEBUG] åŸºç¡€SQLæ„å»ºå®Œæˆ")
        
        # 4. åŠ¨æ€æ·»åŠ ç­›é€‰æ¡ä»¶ - âœ… æ ¸å¿ƒä¿®å¤ï¼šæ£€æŸ¥ None å’Œç©ºå­—ç¬¦ä¸²
        filters = []
        params = {}
        
        # æ—¥æœŸç­›é€‰
        date_start = request.args.get('date_start')
        print(f"ğŸ“Š [DEBUG] date_start åŸå§‹å€¼: {repr(date_start)}")
        if date_start and date_start.strip():  # ç¡®ä¿ä¸æ˜¯ None ä¸”ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
            filters.append("experiment_date >= :date_start")
            params['date_start'] = date_start
            print(f"âœ… [DEBUG] æ·»åŠ  date_start ç­›é€‰: {date_start}")
        
        date_end = request.args.get('date_end')
        print(f"ğŸ“Š [DEBUG] date_end åŸå§‹å€¼: {repr(date_end)}")
        if date_end and date_end.strip():
            filters.append("experiment_date <= :date_end")
            params['date_end'] = date_end
            print(f"âœ… [DEBUG] æ·»åŠ  date_end ç­›é€‰: {date_end}")
        
        # PI è†œå‹å·ç­›é€‰
        pi_film_model = request.args.get('pi_film_model')
        print(f"ğŸ“Š [DEBUG] pi_film_model åŸå§‹å€¼: {repr(pi_film_model)}")
        if pi_film_model:
            # è¿‡æ»¤æ‰ç©ºé¡¹
            models = [m.strip() for m in pi_film_model.split(',') if m.strip()]
            print(f"ğŸ“Š [DEBUG] è§£æåçš„ models: {models}")
            if models:
                placeholders = ','.join([f':model_{i}' for i in range(len(models))])
                filters.append(f"pi_film_model IN ({placeholders})")
                for i, model in enumerate(models):
                    params[f'model_{i}'] = model
                print(f"âœ… [DEBUG] æ·»åŠ  pi_film_model ç­›é€‰: {models}")
        
        # çƒ§åˆ¶åœ°ç‚¹ç­›é€‰
        sintering_location = request.args.get('sintering_location')
        print(f"ğŸ“Š [DEBUG] sintering_location åŸå§‹å€¼: {repr(sintering_location)}")
        if sintering_location:
            locations = [l.strip() for l in sintering_location.split(',') if l.strip()]
            print(f"ğŸ“Š [DEBUG] è§£æåçš„ locations: {locations}")
            if locations:
                placeholders = ','.join([f':location_{i}' for i in range(len(locations))])
                filters.append(f"sintering_location IN ({placeholders})")
                for i, location in enumerate(locations):
                    params[f'location_{i}'] = location
                print(f"âœ… [DEBUG] æ·»åŠ  sintering_location ç­›é€‰: {locations}")
        
        # æ‹¼æ¥ SQL
        if filters:
            query += " AND " + " AND ".join(filters)
            print(f"âœ… [DEBUG] æ·»åŠ äº† {len(filters)} ä¸ªç­›é€‰æ¡ä»¶")
        else:
            print("â„¹ï¸ [DEBUG] æ— é¢å¤–ç­›é€‰æ¡ä»¶")
        
        # ===== æ‰“å°æœ€ç»ˆSQL =====
        print("=" * 60)
        print("ğŸ“Š [DEBUG] æœ€ç»ˆSQLæŸ¥è¯¢:")
        print(query)
        print(f"ğŸ“Š [DEBUG] å‚æ•°å­—å…¸: {params}")
        print("=" * 60)
        
        # 5. æ‰§è¡ŒæŸ¥è¯¢
        try:
            result = db.session.execute(text(query), params)
            raw_data = [dict(row._mapping) for row in result]
            print(f"âœ… [DEBUG] SQLæ‰§è¡ŒæˆåŠŸï¼Œè¿”å› {len(raw_data)} æ¡æ•°æ®")
            
            if len(raw_data) > 0:
                print(f"ğŸ“Š [DEBUG] ç¬¬ä¸€æ¡æ•°æ®ç¤ºä¾‹: {raw_data[0]}")
        except Exception as sql_error:
            print("=" * 60)
            print(f"âŒ [DEBUG] SQLæ‰§è¡Œå¤±è´¥")
            print(f"   é”™è¯¯ç±»å‹: {type(sql_error).__name__}")
            print(f"   é”™è¯¯ä¿¡æ¯: {str(sql_error)}")
            print("=" * 60)
            import traceback
            traceback.print_exc()
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸
        
        # 6. åç»­æ•°æ®æ¸…æ´—é€»è¾‘
        exclude_zero = request.args.get('exclude_zero', 'true').lower() == 'true'
        enable_outlier = request.args.get('enable_outlier_detection', 'true').lower() == 'true'
        outlier_method = request.args.get('outlier_method', 'iqr')
        
        print(f"ğŸ“Š [DEBUG] æ•°æ®æ¸…æ´—å‚æ•°: exclude_zero={exclude_zero}, enable_outlier={enable_outlier}, method={outlier_method}")
        
        cleaned_result = clean_analysis_data(
            raw_data,
            exclude_zero=exclude_zero,
            enable_outlier_detection=enable_outlier,
            outlier_method=outlier_method
        )
        
        print(f"âœ… [DEBUG] æ•°æ®æ¸…æ´—å®Œæˆï¼Œæœ‰æ•ˆæ•°æ®: {len(cleaned_result['data'])} æ¡")
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        cleaning_report = generate_cleaning_report(cleaned_result['statistics'])
        
        print("âœ… [DEBUG] åˆ†ææŠ¥å‘Šç”ŸæˆæˆåŠŸ")
        print("=" * 60)
        
        return jsonify({
            'data': cleaned_result['data'],
            'metadata': {
                'x_field': x_field,
                'x_label': FIELD_METADATA[x_field]['label'],
                'x_unit': FIELD_METADATA[x_field]['unit'],
                'y_field': y_field,
                'y_label': FIELD_METADATA[y_field]['label'],
                'y_unit': FIELD_METADATA[y_field]['unit']
            },
            'statistics': cleaned_result['statistics'],
            'cleaning_report': cleaning_report
        }), 200
    
    except Exception as e:
        # ===== å®Œæ•´çš„é”™è¯¯å¤„ç† =====
        import traceback
        print("=" * 60)
        print("âŒ [DEBUG] æ•°æ®åˆ†ææŸ¥è¯¢å¤±è´¥")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
        print("   å®Œæ•´å †æ ˆ:")
        traceback.print_exc()
        print("=" * 60)
        
        return jsonify({
            'error': 'Data retrieval failed',
            'message': str(e),
            'error_type': type(e).__name__
        }), 500


@analysis_bp.route('/linear-regression', methods=['POST'])
@jwt_required()
@role_required(['admin', 'engineer'])
def linear_regression():
    """
    æ‰§è¡Œçº¿æ€§å›å½’åˆ†æ
    
    Request Body:
        {
            "data": [
                {"x": 2400, "y": 1050},
                {"x": 2600, "y": 1280},
                ...
            ]
        }
    
    Returns:
        {
            "equation": "y = 0.52x - 195.6",
            "slope": 0.52,
            "intercept": -195.6,
            "r_squared": 0.956,
            "p_value": 0.0001,
            "n": 25,
            "predictions": [...],
            "quality_assessment": {...}
        }
    """
    try:
        # 1. è·å–æ•°æ®
        request_data = request.get_json()
        data_points = request_data.get('data', [])
        
        if not data_points:
            return jsonify({
                'error': 'No data provided',
                'message': 'æ²¡æœ‰æä¾›æ•°æ®ç‚¹'
            }), 400
        
        # 2. æå–Xå’ŒYå€¼ï¼ˆåªä½¿ç”¨æœ‰æ•ˆæ•°æ®ï¼‰
        valid_points = [p for p in data_points if isinstance(p, dict) and 'x' in p and 'y' in p]
        
        if len(valid_points) < 2:
            return jsonify({
                'error': 'Insufficient data',
                'message': 'æ•°æ®ç‚¹ä¸è¶³ï¼Œè‡³å°‘éœ€è¦2ä¸ªç‚¹è¿›è¡Œå›å½’åˆ†æ',
                'data_count': len(valid_points)
            }), 400
        
        x_values = np.array([p['x'] for p in valid_points], dtype=float)
        y_values = np.array([p['y'] for p in valid_points], dtype=float)
        
        # 3. è¾¹ç¼˜æƒ…å†µæ£€æŸ¥
        # æ£€æŸ¥Xå€¼æ˜¯å¦æœ‰å˜åŒ–
        if np.all(x_values == x_values[0]):
            return jsonify({
                'error': 'No variance in X',
                'message': 'Xè½´æ•°æ®æ— å˜åŒ–ï¼Œæ— æ³•è®¡ç®—å›å½’æ–¹ç¨‹',
                'x_value': float(x_values[0])
            }), 400
        
        # æ£€æŸ¥Yå€¼æ˜¯å¦æœ‰å˜åŒ–
        if np.all(y_values == y_values[0]):
            return jsonify({
                'error': 'No variance in Y',
                'message': 'Yè½´æ•°æ®æ— å˜åŒ–ï¼Œæ— æ³•è®¡ç®—å›å½’æ–¹ç¨‹',
                'y_value': float(y_values[0])
            }), 400
        
        # 4. æ‰§è¡Œçº¿æ€§å›å½’
        try:
            slope, intercept, r_value, p_value, std_err = stats.linregress(x_values, y_values)
        except Exception as e:
            return jsonify({
                'error': 'Regression calculation failed',
                'message': f'å›å½’è®¡ç®—å¤±è´¥: {str(e)}'
            }), 500
        
        # 5. è®¡ç®—RÂ²
        r_squared = r_value ** 2
        
        # 6. ç”Ÿæˆå›å½’æ–¹ç¨‹å­—ç¬¦ä¸²
        if intercept >= 0:
            equation = f"y = {slope:.4f}x + {intercept:.4f}"
        else:
            equation = f"y = {slope:.4f}x - {abs(intercept):.4f}"
        
        # 7. ç”Ÿæˆé¢„æµ‹ç‚¹ï¼ˆç”¨äºç»˜åˆ¶å›å½’çº¿ï¼‰
        x_min, x_max = np.min(x_values), np.max(x_values)
        x_range = x_max - x_min
        x_pred = np.linspace(x_min - 0.1 * x_range, x_max + 0.1 * x_range, 50)
        y_pred = slope * x_pred + intercept
        
        predictions = [
            {'x': float(x), 'y': float(y)}
            for x, y in zip(x_pred, y_pred)
        ]
        
        # 8. è´¨é‡è¯„ä¼°
        quality_assessment = {
            'fit_quality': _assess_fit_quality(r_squared),
            'significance': _assess_significance(p_value)
        }
        
        # 9. è¿”å›ç»“æœ
        return jsonify({
            'equation': equation,
            'slope': float(slope),
            'intercept': float(intercept),
            'r_squared': float(r_squared),
            'p_value': float(p_value),
            'std_err': float(std_err),
            'n': len(valid_points),
            'predictions': predictions,
            'quality_assessment': quality_assessment
        }), 200
    
    except Exception as e:
        return jsonify({
            'error': 'Analysis failed',
            'message': str(e)
        }), 500


def _assess_fit_quality(r_squared: float) -> str:
    """è¯„ä¼°æ‹Ÿåˆè´¨é‡"""
    if r_squared >= 0.9:
        return 'excellent'
    elif r_squared >= 0.75:
        return 'good'
    elif r_squared >= 0.5:
        return 'fair'
    else:
        return 'poor'


def _assess_significance(p_value: float) -> str:
    """è¯„ä¼°æ˜¾è‘—æ€§"""
    if p_value < 0.001:
        return 'highly_significant'
    elif p_value < 0.05:
        return 'moderately_significant'
    else:
        return 'not_significant'


@analysis_bp.route('/field-options', methods=['GET'])
@jwt_required()
def get_field_options():
    """
    è·å–å¯ç”¨äºåˆ†æçš„å­—æ®µåˆ—è¡¨
    
    Returns:
        {
            'fields': [
                {
                    'value': 'graphite_max_temp',
                    'label': 'çŸ³å¢¨åŒ–æœ€é«˜æ¸©åº¦',
                    'unit': 'â„ƒ',
                    'category': 'process'
                },
                ...
            ]
        }
    """
    fields = []
    
    # åˆ†ç±»å®šä¹‰
    categories = {
        'carbonization': 'ç¢³åŒ–å‚æ•°',
        'graphitization': 'çŸ³å¢¨åŒ–å‚æ•°',
        'product': 'æˆå“å‚æ•°',
        'pi_film': 'PIè†œå‚æ•°',
        'rolling': 'å‹å»¶å‚æ•°'
    }
    
    # å­—æ®µåˆ†ç±»
    field_categories = {
        'carbon_max_temp': 'carbonization',
        'carbon_total_time': 'carbonization',
        'carbon_yield_rate': 'carbonization',
        
        'graphite_max_temp': 'graphitization',
        'graphite_total_time': 'graphitization',
        'graphite_yield_rate': 'graphitization',
        
        'thermal_conductivity': 'product',
        'avg_density': 'product',
        'avg_thickness': 'product',
        'shrinkage_ratio': 'product',
        'cohesion': 'product',
        'peel_strength': 'product',
        
        'pi_film_thickness': 'pi_film'
    }
    
    for field_name, metadata in FIELD_METADATA.items():
        fields.append({
            'value': field_name,
            'label': metadata['label'],
            'unit': metadata['unit'],
            'category': field_categories.get(field_name, 'other'),
            'category_label': categories.get(field_categories.get(field_name, 'other'), 'å…¶ä»–')
        })
    
    return jsonify({'fields': fields}), 200
